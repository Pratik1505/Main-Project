# -*- coding: utf-8 -*-
"""Spark_ML_model (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14j_RIPLHtYfD2n1vQRAzq6F7FEHbi7ue
"""

#Install Java Development kit for Spark
!apt-get install openjdk-8-jdk

import os

#Set the JAVA_HOME env variable
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-8-openjdk-amd64"

#Current working directory
!pwd

!echo $JAVA_HOME

#Install PySpark with latest version
!pip install pyspark==3.0.0

#mount your drive
from google.colab import drive
drive.mount('/content/drive')

#Create object of spark session
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.sql import SparkSession, Row
#import Row appName= "hive_pyspark" master= "local"
#spark = SparkSession.builder.appName('ml-bank').enableHiveSupport().getOrCreate()
spark = SparkSession.builder.master("local").appName('Read Data from hive and show').enableHiveSupport().getOrCreate()

#Read the csv file
df=spark.read.csv('/content/world_energy_consumption.csv',inferSchema=True,header=True)

from pyspark.sql.functions import regexp_replace,trim

# Define a list of suffixes to remove
suffixes = ["\(Ember\)", "\(EIA\)", "\(EI\)", "\(Shift\)", "\(27\)", "\(country\)"]

# Iterate over each suffix and remove it from the "country" column
for suffix in suffixes:
    df = df.withColumn("country", regexp_replace(df["country"], suffix, ""))

# Trim leading and trailing whitespace from the "country" column
df = df.withColumn("country", trim(df["country"]))

#df=spark.sql("SELECT * FROM project.energy")

df.show(10)

df.printSchema()

# Display some sample data to inspect the 'country' column
df.select("country").show(5)

from pyspark.ml.feature import  StringIndexer

# Define label and feature columns
label = 'greenhouse_gas_emissions'
feature_columns = [col for col in df.columns if col != label]

# Drop rows with null values in the 'country' column
#df = df.dropna(subset=["country"])

# Apply StringIndexer to 'country' column
string_indexer = StringIndexer(inputCol="country", outputCol="country_index")
df_indexed = string_indexer.fit(df).transform(df)

df_indexed.show(5)

df_indexed = df_indexed.drop('country')
df_indexed.show(5)

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression, GBTRegressor
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

# Assemble features into a vector
assembler = VectorAssembler(inputCols=[col for col in df_indexed.columns if col != 'greenhouse_gas_emissions'],
                            outputCol="features")

# Define Linear Regression model
lr = LinearRegression(featuresCol="features", labelCol="greenhouse_gas_emissions")

# Define Gradient Boosting Regressor model with increased maxBins
gbt = GBTRegressor(featuresCol="features", labelCol="greenhouse_gas_emissions", maxBins=300)

# Create pipelines
pipeline_lr = Pipeline(stages=[assembler, lr])
pipeline_gbt = Pipeline(stages=[assembler, gbt])

# Split the data into training and testing sets
(training_data, testing_data) = df_indexed.randomSplit([0.7, 0.3])

# Train the Linear Regression model
model_lr = pipeline_lr.fit(training_data)

# Train the Gradient Boosting Regressor model
model_gbt = pipeline_gbt.fit(training_data)

# Make predictions
predictions_lr = model_lr.transform(testing_data)
predictions_gbt = model_gbt.transform(testing_data)

# Evaluate the Linear Regression model
evaluator_lr = RegressionEvaluator(labelCol="greenhouse_gas_emissions", predictionCol="prediction", metricName="rmse")
rmse_lr = evaluator_lr.evaluate(predictions_lr)
print("Linear Regression - Root Mean Squared Error (RMSE) on test data:", rmse_lr)

# Evaluate the Gradient Boosting Regressor model
evaluator_gbt = RegressionEvaluator(labelCol="greenhouse_gas_emissions", predictionCol="prediction", metricName="rmse")
rmse_gbt = evaluator_gbt.evaluate(predictions_gbt)
print("Gradient Boosting Regressor - Root Mean Squared Error (RMSE) on test data:", rmse_gbt)

import matplotlib.pyplot as plt

# Extract predicted and actual values for Linear Regression model
predicted_lr = predictions_lr.select("prediction").rdd.map(lambda row: row[0]).collect()
actual_lr = predictions_lr.select("greenhouse_gas_emissions").rdd.map(lambda row: row[0]).collect()

# Extract predicted and actual values for Gradient Boosting Regressor model
predicted_gbt = predictions_gbt.select("prediction").rdd.map(lambda row: row[0]).collect()
actual_gbt = predictions_gbt.select("greenhouse_gas_emissions").rdd.map(lambda row: row[0]).collect()

# Create scatter plot for Linear Regression model
plt.figure(figsize=(10, 6))
plt.scatter(actual_lr, predicted_lr, color='blue', label='Linear Regression')
plt.xlabel('Actual Greenhouse Gas Emissions')
plt.ylabel('Predicted Greenhouse Gas Emissions')
plt.title('Linear Regression: Predicted vs. Actual Greenhouse Gas Emissions')
plt.legend()
plt.grid(True)
plt.show()

# Create scatter plot for Gradient Boosting Regressor model
plt.figure(figsize=(10, 6))
plt.scatter(actual_gbt, predicted_gbt, color='green', label='Gradient Boosting Regressor')
plt.xlabel('Actual Greenhouse Gas Emissions')
plt.ylabel('Predicted Greenhouse Gas Emissions')
plt.title('Gradient Boosting Regressor: Predicted vs. Actual Greenhouse Gas Emissions')
plt.legend()
plt.grid(True)
plt.show()

df_indexed.write.csv("file:///C:/Users/Dell/OneDrive/Desktop/newWorld_EnergyConsumption.csv", header=True, mode="overwrite")